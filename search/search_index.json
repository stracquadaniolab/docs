{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Laboratory documentation Welcome to the Stracquadanio lab documentation website. If you just joined the lab, the best way to use the documentation is to follow to go through the sections systems setup , project lifecyle and coding standards in this exact order. Useful links Lab website: https://www.stracquadaniolab.org Docs: https://www.stracquadaniolab.org/docs GitHub: https://github.com/stracquadaniolab","title":"Home"},{"location":"#laboratory-documentation","text":"Welcome to the Stracquadanio lab documentation website. If you just joined the lab, the best way to use the documentation is to follow to go through the sections systems setup , project lifecyle and coding standards in this exact order.","title":"Laboratory documentation"},{"location":"#useful-links","text":"Lab website: https://www.stracquadaniolab.org Docs: https://www.stracquadaniolab.org/docs GitHub: https://github.com/stracquadaniolab","title":"Useful links"},{"location":"programming/coding-standards/","text":"Programming languages The default programming language for all our project is Python version 3.7 or newer. Your code should be compliant to PEP8 and formatted using black . Object Oriented Programming (OOP) should be used whenever possible. For specific tasks, such as preparing figures for publication or differential expression analysis, we might rely on R . Programming environments The default integrated development environment (IDE) is Visual Studio Code (aka vscode ). All code must be developed using vscode . Workflow systems Project workflows must be implemented in Nextflow . You are encouraged to use Tower to monitor your experiments. Software environments It is highly recommended to develop your code in isolated Docker containers using vscode and install required software using conda . The workflow cookiecutter comes preconfigured to run on Docker/Singularity containers. Readme and manifest files Readme and manifest files MUST be written in Markdown. Backup your project workspace A project workspace must be backed up using duplicacy right after you finish your experiments. Duplicacy performs versioning and bit-level deduplication, in order to minimise storage space and data transfers.","title":"Coding standards"},{"location":"programming/coding-standards/#programming-languages","text":"The default programming language for all our project is Python version 3.7 or newer. Your code should be compliant to PEP8 and formatted using black . Object Oriented Programming (OOP) should be used whenever possible. For specific tasks, such as preparing figures for publication or differential expression analysis, we might rely on R .","title":"Programming languages"},{"location":"programming/coding-standards/#programming-environments","text":"The default integrated development environment (IDE) is Visual Studio Code (aka vscode ). All code must be developed using vscode .","title":"Programming environments"},{"location":"programming/coding-standards/#workflow-systems","text":"Project workflows must be implemented in Nextflow . You are encouraged to use Tower to monitor your experiments.","title":"Workflow systems"},{"location":"programming/coding-standards/#software-environments","text":"It is highly recommended to develop your code in isolated Docker containers using vscode and install required software using conda . The workflow cookiecutter comes preconfigured to run on Docker/Singularity containers.","title":"Software environments"},{"location":"programming/coding-standards/#readme-and-manifest-files","text":"Readme and manifest files MUST be written in Markdown.","title":"Readme and manifest files"},{"location":"programming/coding-standards/#backup-your-project-workspace","text":"A project workspace must be backed up using duplicacy right after you finish your experiments. Duplicacy performs versioning and bit-level deduplication, in order to minimise storage space and data transfers.","title":"Backup your project workspace"},{"location":"programming/docker/","text":"Docker GPU is not used in training Neural Networks (NNs) : 1) In order to enable GPU for NN training it is not enough to push model and training data to GPU (i.e. model = model.cuda() or model.to(device) where device is \"cuda\"). The stable and error-proof way for using GPU is to run your nextflow (NF) under a specific profile (i.e. nextflow run stracquadaniolab/ -profile dockergpu). Here is a necessary configuration: dockergpu { docker.enabled = true docker.runOptions = \"--gpus all --ipc=host\" } As you can see the docker is enabled, which means that a proper docker image must be built in order to allow GPU to be used in NN training. This brings us to a particular configuration of Dockerfile: # to allow GPU inherit from pytorch-geometric FROM ghcr.io/stracquadaniolab/pytorch-geometric:latest LABEL org.stracquadaniolab.name=\"<workflow_name>\" LABEL org.stracquadaniolab.description=\"<workflow_description>\" RUN apt-get update \\ && apt-get install --yes rename procps curl\\ && apt-get autoremove \\ && apt-get clean \\ && rm -rf /var/lib/apt/lists/* WORKDIR / COPY environment.yml / RUN conda env update -n base --file environment.yml && conda clean --all --yes ENV TINI_VERSION v0.19.0 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [\"tini\", \"--\"] CMD [\"/bin/bash\"] After these changes are incorporated you should be able to run you workflow. Command will look like (also pay attention to FAQ section about 137 error and modify sge profile or supply your own): nextflow run stracquadaniolab/<your_workflow_name> -profile test, dockergpu, sge","title":"Docker"},{"location":"programming/docker/#docker","text":"GPU is not used in training Neural Networks (NNs) : 1) In order to enable GPU for NN training it is not enough to push model and training data to GPU (i.e. model = model.cuda() or model.to(device) where device is \"cuda\"). The stable and error-proof way for using GPU is to run your nextflow (NF) under a specific profile (i.e. nextflow run stracquadaniolab/ -profile dockergpu). Here is a necessary configuration: dockergpu { docker.enabled = true docker.runOptions = \"--gpus all --ipc=host\" } As you can see the docker is enabled, which means that a proper docker image must be built in order to allow GPU to be used in NN training. This brings us to a particular configuration of Dockerfile: # to allow GPU inherit from pytorch-geometric FROM ghcr.io/stracquadaniolab/pytorch-geometric:latest LABEL org.stracquadaniolab.name=\"<workflow_name>\" LABEL org.stracquadaniolab.description=\"<workflow_description>\" RUN apt-get update \\ && apt-get install --yes rename procps curl\\ && apt-get autoremove \\ && apt-get clean \\ && rm -rf /var/lib/apt/lists/* WORKDIR / COPY environment.yml / RUN conda env update -n base --file environment.yml && conda clean --all --yes ENV TINI_VERSION v0.19.0 ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini RUN chmod +x /tini ENTRYPOINT [\"tini\", \"--\"] CMD [\"/bin/bash\"] After these changes are incorporated you should be able to run you workflow. Command will look like (also pay attention to FAQ section about 137 error and modify sge profile or supply your own): nextflow run stracquadaniolab/<your_workflow_name> -profile test, dockergpu, sge","title":"Docker"},{"location":"projects-management/project-lifecycle/","text":"Project lifecycle We have defined a project lifecycle to make our experiments reproducible. A project is made of two components: the workflow: that is all your code to perform analysis and run experiments. the workspace: that is a directory on a cluster or workstation, where all your experiments take place. We have defined templates, called cookiecutters , to setup a workflow and a workspace according to our standards. Managing your project is a 3 step process: graph LR A[develop a workflow] --> B[run experiments] B --> C[write a report] C --> A Develop a workflow Create a workflow using cookiecutter-workflow-nf . Please, read how to prepare your workflow here . Write the code for your workflow. Add test data and package required software with Docker. Push your workflow to GitHub. Run experiments Create a project workspace using the cookiecutter-workspace-nf . Please, read carefully how the workspace is organized here . Pull your workflow from GitHub. Set your experimental parameters. Run your experiments. Reporting Create a report using the cookiecutter-report-nf Write your report including: methods used and/or implemented; experimental setup, e.g. parameters, workflow version etc. tables and figures","title":"Project lifecycle"},{"location":"projects-management/project-lifecycle/#project-lifecycle","text":"We have defined a project lifecycle to make our experiments reproducible. A project is made of two components: the workflow: that is all your code to perform analysis and run experiments. the workspace: that is a directory on a cluster or workstation, where all your experiments take place. We have defined templates, called cookiecutters , to setup a workflow and a workspace according to our standards. Managing your project is a 3 step process: graph LR A[develop a workflow] --> B[run experiments] B --> C[write a report] C --> A","title":"Project lifecycle"},{"location":"projects-management/project-lifecycle/#develop-a-workflow","text":"Create a workflow using cookiecutter-workflow-nf . Please, read how to prepare your workflow here . Write the code for your workflow. Add test data and package required software with Docker. Push your workflow to GitHub.","title":"Develop a workflow"},{"location":"projects-management/project-lifecycle/#run-experiments","text":"Create a project workspace using the cookiecutter-workspace-nf . Please, read carefully how the workspace is organized here . Pull your workflow from GitHub. Set your experimental parameters. Run your experiments.","title":"Run experiments"},{"location":"projects-management/project-lifecycle/#reporting","text":"Create a report using the cookiecutter-report-nf Write your report including: methods used and/or implemented; experimental setup, e.g. parameters, workflow version etc. tables and figures","title":"Reporting"},{"location":"projects-management/project-workflow/","text":"Project workflows We use a standardized workflow structure based on Nextflow, which simplify developing methods and run experiments across different environments. Create a new workflow structure cookiecutter https://github.com/stracquadaniolab/cookiecutter-workflow-nf.git The system will ask you a few questions and then create the structure for you. Successively, create a repo in GitHub with the name of the directory just created. Directory structure \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 ci.yml \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 fit.py \u2502 \u2514\u2500\u2500 plots.py \u251c\u2500\u2500 conf \u2502 \u2514\u2500\u2500 base.config \u251c\u2500\u2500 containers \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u2514\u2500\u2500 environment.yml \u251c\u2500\u2500 testdata \u2502 \u2514\u2500\u2500 mydata.txt \u251c\u2500\u2500 .bumpversion.cfg \u251c\u2500\u2500 .devcontainer.json \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 main.nf \u251c\u2500\u2500 nextflow.config \u2514\u2500\u2500 readme.md The workflow file The main.nf file contains the entrypoint for the workflow, and it uses Nextflow DSL2 by default. The workflow parameters are stored in the nextflow.config file, which in turn include other files in the conf directory; usually, you only have to define the parameters of your specific pipeline, since the conf/base.conf file includes profiles to run your workflow in different computing environment, e.g. Slurm, GitHub. Please, refer to the Nextflow documentation for an overview of the framework. Custom scripts management Custom code (aka your scripts and classes) needed by the pipeline should be added to the bin directory; the code in this directory is automatically added to $PATH when running the pipeline, which makes custom scripts easily portable and accessible. If you are using Python, you should have a file for each class of operations, e.g. a file plots.py for all the plots, and use docopt to have standard Unix command line interface. See the auto-generated pipeline for an example. Software management Third-party software is managed by micromamba and specified in a environment.yml file; keep the yml file updated and specify the version of each software you are using in order to ensure reproducibility. To ensure reproducibility and running experiments on local machines and HPC clusters, it is strongly recommended to build a Docker image. The bundled Dockerfile can be used to build an image with the software specified in your environment.yml file. To do that, run: docker build . -t ghcr.io/stracquadaniolab/<my-workflow>:<version> -f containers/Dockerfile where <my-workflow> is the name of your workflow and <version> is the current version of your workflow, starting from 0.0.0 . The template comes with an auto-generated .devcontainer.json file, which allows developing your scripts inside a container with all the software specified in environment.yml using vscode . Sometimes you would want to pull a docker image from GitHub container registry: docker pull ghcr.io/stracquadaniolab/<workflow_name>:<version> In order to successfully pull an image, first you need to authenticate yourself with your personal access token, see here: Authenticating with the container registry Testing It is important to build workflows that can be automatically tested; thus, you will have to add small test data into the testdata directory, and modify the test profile in conf/base.config configuration file to specify any parameter needed for your workflow to run. See the auto-generated pipeline for an example. Versioning All projects must follow a semantic version scheme. The format adopted is MAJOR.MINOR.PATCH : MAJOR: drastic changes that make disruptive changes with a previous release. MINOR: add functions to the workflow but keeps everything compatible within the MAJOR version. PATCH: bug fixes or settings update. To update the version of your workflow, you should run the following command from the command line: bump2version major #for major release bump2version minor #for minor release bump2version patch #for patch release Push your code to GitHub As the project is version controlled using Git, you can push your code to GitHub as follows: git add . git commit -am \"new: added super cool feature\" git push -u origin master Importantly, after a bumpversion , you also have to push the tag just created as follows: git push --tags Continuous integration Each pipeline comes with a pre-configured GitHub workflow to automatically test the code and build a Docker image; the workflow is stored in .github/workflows/ci.yml . Please note that a Docker image is only released when you push a tag. Documentation Each workflow must have an updated readme.md file, describing: what the workflow does how to configure the workflow how to run the workflow a description of the output generated A readme.md file with the required sections is automatically generated by this cookiecutter.","title":"Project workflows"},{"location":"projects-management/project-workflow/#project-workflows","text":"We use a standardized workflow structure based on Nextflow, which simplify developing methods and run experiments across different environments.","title":"Project workflows"},{"location":"projects-management/project-workflow/#create-a-new-workflow-structure","text":"cookiecutter https://github.com/stracquadaniolab/cookiecutter-workflow-nf.git The system will ask you a few questions and then create the structure for you. Successively, create a repo in GitHub with the name of the directory just created.","title":"Create a new workflow structure"},{"location":"projects-management/project-workflow/#directory-structure","text":"\u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 ci.yml \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 fit.py \u2502 \u2514\u2500\u2500 plots.py \u251c\u2500\u2500 conf \u2502 \u2514\u2500\u2500 base.config \u251c\u2500\u2500 containers \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u2514\u2500\u2500 environment.yml \u251c\u2500\u2500 testdata \u2502 \u2514\u2500\u2500 mydata.txt \u251c\u2500\u2500 .bumpversion.cfg \u251c\u2500\u2500 .devcontainer.json \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 main.nf \u251c\u2500\u2500 nextflow.config \u2514\u2500\u2500 readme.md","title":"Directory structure"},{"location":"projects-management/project-workflow/#the-workflow-file","text":"The main.nf file contains the entrypoint for the workflow, and it uses Nextflow DSL2 by default. The workflow parameters are stored in the nextflow.config file, which in turn include other files in the conf directory; usually, you only have to define the parameters of your specific pipeline, since the conf/base.conf file includes profiles to run your workflow in different computing environment, e.g. Slurm, GitHub. Please, refer to the Nextflow documentation for an overview of the framework.","title":"The workflow file"},{"location":"projects-management/project-workflow/#custom-scripts-management","text":"Custom code (aka your scripts and classes) needed by the pipeline should be added to the bin directory; the code in this directory is automatically added to $PATH when running the pipeline, which makes custom scripts easily portable and accessible. If you are using Python, you should have a file for each class of operations, e.g. a file plots.py for all the plots, and use docopt to have standard Unix command line interface. See the auto-generated pipeline for an example.","title":"Custom scripts management"},{"location":"projects-management/project-workflow/#software-management","text":"Third-party software is managed by micromamba and specified in a environment.yml file; keep the yml file updated and specify the version of each software you are using in order to ensure reproducibility. To ensure reproducibility and running experiments on local machines and HPC clusters, it is strongly recommended to build a Docker image. The bundled Dockerfile can be used to build an image with the software specified in your environment.yml file. To do that, run: docker build . -t ghcr.io/stracquadaniolab/<my-workflow>:<version> -f containers/Dockerfile where <my-workflow> is the name of your workflow and <version> is the current version of your workflow, starting from 0.0.0 . The template comes with an auto-generated .devcontainer.json file, which allows developing your scripts inside a container with all the software specified in environment.yml using vscode . Sometimes you would want to pull a docker image from GitHub container registry: docker pull ghcr.io/stracquadaniolab/<workflow_name>:<version> In order to successfully pull an image, first you need to authenticate yourself with your personal access token, see here: Authenticating with the container registry","title":"Software management"},{"location":"projects-management/project-workflow/#testing","text":"It is important to build workflows that can be automatically tested; thus, you will have to add small test data into the testdata directory, and modify the test profile in conf/base.config configuration file to specify any parameter needed for your workflow to run. See the auto-generated pipeline for an example.","title":"Testing"},{"location":"projects-management/project-workflow/#versioning","text":"All projects must follow a semantic version scheme. The format adopted is MAJOR.MINOR.PATCH : MAJOR: drastic changes that make disruptive changes with a previous release. MINOR: add functions to the workflow but keeps everything compatible within the MAJOR version. PATCH: bug fixes or settings update. To update the version of your workflow, you should run the following command from the command line: bump2version major #for major release bump2version minor #for minor release bump2version patch #for patch release","title":"Versioning"},{"location":"projects-management/project-workflow/#push-your-code-to-github","text":"As the project is version controlled using Git, you can push your code to GitHub as follows: git add . git commit -am \"new: added super cool feature\" git push -u origin master Importantly, after a bumpversion , you also have to push the tag just created as follows: git push --tags","title":"Push your code to GitHub"},{"location":"projects-management/project-workflow/#continuous-integration","text":"Each pipeline comes with a pre-configured GitHub workflow to automatically test the code and build a Docker image; the workflow is stored in .github/workflows/ci.yml . Please note that a Docker image is only released when you push a tag.","title":"Continuous integration"},{"location":"projects-management/project-workflow/#documentation","text":"Each workflow must have an updated readme.md file, describing: what the workflow does how to configure the workflow how to run the workflow a description of the output generated A readme.md file with the required sections is automatically generated by this cookiecutter.","title":"Documentation"},{"location":"projects-management/project-workspace/","text":"Project workspaces We use a standardized directory structure, called workspace, where we run and store all our experiments for a given project. Create a workspace cookiecutter https://github.com/stracquadaniolab/cookiecutter-workspace-nf.git The system will ask you a few questions and then create the structure for you. Directory structure . \u251c\u2500\u2500 conf \u251c\u2500\u2500 data \u251c\u2500\u2500 logs \u251c\u2500\u2500 resources \u251c\u2500\u2500 results \u2514\u2500\u2500 readme.md The conf directory contains Nextflow config files to run a pipeline; you must define the parameters of each experiment in a config file rather than passing them on the command line. The data directory contains data to be processed by a pipeline. This directory usually contains the raw data (e.g. data from sequencing experiments). You should take some time to organize it in a meaningful and consistent way. The resources directory contains data retrieved from external sources/repositories, like annotation files (e.g. genome GFF) or geneset GMT files. The logs directory contains the log of each pipeline run. The results directory contains the result of experiments. You should take some time to organize it in a meaningful and consistent way. It is strongly recommended to results in a directory named like 2022-01-01-my-first-test . The readme.md file contains a description of the project and how the data and results folder are organized. Naming guidelines The team MUST use the Google naming guidelines, specifically: Make file and directory names lowercase. Separate words with hyphens, not underscores. Use only standard ASCII alphanumeric characters in file and directory names. IMPORTANT : Raw data or external resources are allowed to keep their naming standard if it makes them easier to identify.","title":"Project workspaces"},{"location":"projects-management/project-workspace/#project-workspaces","text":"We use a standardized directory structure, called workspace, where we run and store all our experiments for a given project.","title":"Project workspaces"},{"location":"projects-management/project-workspace/#create-a-workspace","text":"cookiecutter https://github.com/stracquadaniolab/cookiecutter-workspace-nf.git The system will ask you a few questions and then create the structure for you.","title":"Create a workspace"},{"location":"projects-management/project-workspace/#directory-structure","text":". \u251c\u2500\u2500 conf \u251c\u2500\u2500 data \u251c\u2500\u2500 logs \u251c\u2500\u2500 resources \u251c\u2500\u2500 results \u2514\u2500\u2500 readme.md The conf directory contains Nextflow config files to run a pipeline; you must define the parameters of each experiment in a config file rather than passing them on the command line. The data directory contains data to be processed by a pipeline. This directory usually contains the raw data (e.g. data from sequencing experiments). You should take some time to organize it in a meaningful and consistent way. The resources directory contains data retrieved from external sources/repositories, like annotation files (e.g. genome GFF) or geneset GMT files. The logs directory contains the log of each pipeline run. The results directory contains the result of experiments. You should take some time to organize it in a meaningful and consistent way. It is strongly recommended to results in a directory named like 2022-01-01-my-first-test . The readme.md file contains a description of the project and how the data and results folder are organized.","title":"Directory structure"},{"location":"projects-management/project-workspace/#naming-guidelines","text":"The team MUST use the Google naming guidelines, specifically: Make file and directory names lowercase. Separate words with hyphens, not underscores. Use only standard ASCII alphanumeric characters in file and directory names. IMPORTANT : Raw data or external resources are allowed to keep their naming standard if it makes them easier to identify.","title":"Naming guidelines"},{"location":"systems-setup/github/","text":"GitHub The source code of all projects, cookiecutters, and Docker images are stored in GitHub, under the stracquadaniolab organization. Account setup Register a GitHub account at http://github.com . To be added to the stracquadaniolab organization, send an email to Giovanni with your GitHub username. Get a Personal Access Token (PAT) GitHub requires a Personal Access Token (PAT) to access repositories programmatically, e.g. when cloning or pulling a repo, or pulling a docker image. PATs are personal and must not be shared with anyone. You can obtain a PAT as follows: Login to GitHub Go to your account Settings > Developer Settings > Personal access tokens , and click on Generate a new token . In the form, put a name, e.g. Personal PAT , and select an expiration date, e.g. 3 years. Then, select the scopes you want to get access for; it is highly recommended you only tick repo , workflow , write:packages . Click Generate token and make sure to save the string, which you will get in the new page and it looks like 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 , since it will be shown only once.","title":"GitHub"},{"location":"systems-setup/github/#github","text":"The source code of all projects, cookiecutters, and Docker images are stored in GitHub, under the stracquadaniolab organization.","title":"GitHub"},{"location":"systems-setup/github/#account-setup","text":"Register a GitHub account at http://github.com . To be added to the stracquadaniolab organization, send an email to Giovanni with your GitHub username.","title":"Account setup"},{"location":"systems-setup/github/#get-a-personal-access-token-pat","text":"GitHub requires a Personal Access Token (PAT) to access repositories programmatically, e.g. when cloning or pulling a repo, or pulling a docker image. PATs are personal and must not be shared with anyone. You can obtain a PAT as follows: Login to GitHub Go to your account Settings > Developer Settings > Personal access tokens , and click on Generate a new token . In the form, put a name, e.g. Personal PAT , and select an expiration date, e.g. 3 years. Then, select the scopes you want to get access for; it is highly recommended you only tick repo , workflow , write:packages . Click Generate token and make sure to save the string, which you will get in the new page and it looks like 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 , since it will be shown only once.","title":"Get a Personal Access Token (PAT)"},{"location":"systems-setup/hpc-cell/","text":"Cell HPC Cell is the lab High Performance Computing (HPC) system, which should be used for high priority jobs, or jobs requiring resources hard to schedule on the EDDIE university system. Available resources Hardware resources 128 CPU cores, 2.4Ghz-3.1GHz 64 or 128 Gb ECC RAM per node 24Tb storage 1 Nvidia TitanX 16GB GPU Software resources Nvidia GPU drivers Docker Singularity Slurm Obtaining access Access to Cell is a granted by informing the Dr Stracquadanio and filling a ticket with the IS Helpline , specifying that you will need to be added to the lab group. Login Cell is available only through SSH within the University of Edinburgh network, including UoE VPN, UoE Eduroam, and office ethernet outlets. To access the login node, use SSH as follows: ssh UUN@cell.bio.ed.ac.uk where UUN is your university UUN and the password is the one used for MyED and the other University services. Filesystems Directory Description Size Backup /localdisk/storage/home/<UUN> Home directory to store conda or basic tools 50Gb No /localdisk/storage/projects Directory to store teams' projects 10Tb Yes /localdisk/storage/datasets Directory to store raw data 5Tb Yes WARNING Although the projects are regularly backed up, Cell is not intended for long term storage. Therefore, it's mandatory to transfer your raw data and final results on the group Datastore regularly. Account setup Install miniconda You should use miniconda to install any software you might need, since it allows user-level installations of most common Unix software. If you cannot find your software in the default channel, please check also the conda-forge and bioconda channels. To install miniconda , download the installer in your home directory as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Then, run the installer as follows: bash Miniconda3-latest-Linux-x86_64.sh and answer 'yes' to each question including the default location. Finally, activate your installation by running: source ~/.bashrc Install cookiecutter You can install cookiecutter by running the following command: conda install -c conda-forge cookiecutter Install nextflow You can install Nextflow from the command line by running: curl -s https://get.nextflow.io | bash Add your GitHub PAT You need to add your PAT to the Nextflow configuration to pull from the lab GitHub organization. To do that, create a new text file as follows: nano ~/.nextflow/scm and write the following configuration parameters, replacing the user and password information with the your credentials. providers { github { user = 'your-user-name' password = 'your-personal-access-token;' } } Finally, press Ctrl+o to write your configuration to file. To pull Docker images from GitHub, you also have to configure the credentials for singularity as environment variables. To do that, create a new config file as follows: nano ~/.nextflow/config and write the following parameters, replacing the SINGULARITY_DOCKER_PASSWORD and with the your PAT. env { SINGULARITY_DOCKER_LOGIN='$oauthtoken' SINGULARITY_DOCKER_PASSWORD='your-personal-access-token;' } Finally, press Ctrl+o to write your configuration to file. Test your installation You can test your installation by creating a test-workflow in your home directory as follows: mkdir ~/test-workflow && cd ~/test-workflow Now pull the boot-nf workflow as follows: nextflow pull stracquadaniolab/boot-nf and run it on Cell as follows: nextflow run stracquadaniolab/boot-nf -profile singularity,slurm,test which will create a file results/results.txt containing your ip address and location in JSON format. FAQ I tried to pull a docker image, but it gives me an auth error : 1) you should have a PAT (personal access token) added so that you can access github images github help . 2) you should be added to the docker group by an admin, contact the IS helpline to have your used enable 3) your docker image should be built and pushed to the gh repository (check the github action for the workflow). These three steps should solve 95% of problems. Pipeline suddenly fails with 137 error : 1) Most likely you haven't allocated enough memory for your job. Check Nextflow configuration file for details (particularly executor scope: memory setting).","title":"Cell HPC"},{"location":"systems-setup/hpc-cell/#cell-hpc","text":"Cell is the lab High Performance Computing (HPC) system, which should be used for high priority jobs, or jobs requiring resources hard to schedule on the EDDIE university system.","title":"Cell HPC"},{"location":"systems-setup/hpc-cell/#available-resources","text":"","title":"Available resources"},{"location":"systems-setup/hpc-cell/#hardware-resources","text":"128 CPU cores, 2.4Ghz-3.1GHz 64 or 128 Gb ECC RAM per node 24Tb storage 1 Nvidia TitanX 16GB GPU","title":"Hardware resources"},{"location":"systems-setup/hpc-cell/#software-resources","text":"Nvidia GPU drivers Docker Singularity Slurm","title":"Software resources"},{"location":"systems-setup/hpc-cell/#obtaining-access","text":"Access to Cell is a granted by informing the Dr Stracquadanio and filling a ticket with the IS Helpline , specifying that you will need to be added to the lab group.","title":"Obtaining access"},{"location":"systems-setup/hpc-cell/#login","text":"Cell is available only through SSH within the University of Edinburgh network, including UoE VPN, UoE Eduroam, and office ethernet outlets. To access the login node, use SSH as follows: ssh UUN@cell.bio.ed.ac.uk where UUN is your university UUN and the password is the one used for MyED and the other University services.","title":"Login"},{"location":"systems-setup/hpc-cell/#filesystems","text":"Directory Description Size Backup /localdisk/storage/home/<UUN> Home directory to store conda or basic tools 50Gb No /localdisk/storage/projects Directory to store teams' projects 10Tb Yes /localdisk/storage/datasets Directory to store raw data 5Tb Yes WARNING Although the projects are regularly backed up, Cell is not intended for long term storage. Therefore, it's mandatory to transfer your raw data and final results on the group Datastore regularly.","title":"Filesystems"},{"location":"systems-setup/hpc-cell/#account-setup","text":"","title":"Account setup"},{"location":"systems-setup/hpc-cell/#install-miniconda","text":"You should use miniconda to install any software you might need, since it allows user-level installations of most common Unix software. If you cannot find your software in the default channel, please check also the conda-forge and bioconda channels. To install miniconda , download the installer in your home directory as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Then, run the installer as follows: bash Miniconda3-latest-Linux-x86_64.sh and answer 'yes' to each question including the default location. Finally, activate your installation by running: source ~/.bashrc","title":"Install miniconda"},{"location":"systems-setup/hpc-cell/#install-cookiecutter","text":"You can install cookiecutter by running the following command: conda install -c conda-forge cookiecutter","title":"Install cookiecutter"},{"location":"systems-setup/hpc-cell/#install-nextflow","text":"You can install Nextflow from the command line by running: curl -s https://get.nextflow.io | bash","title":"Install nextflow"},{"location":"systems-setup/hpc-cell/#add-your-github-pat","text":"You need to add your PAT to the Nextflow configuration to pull from the lab GitHub organization. To do that, create a new text file as follows: nano ~/.nextflow/scm and write the following configuration parameters, replacing the user and password information with the your credentials. providers { github { user = 'your-user-name' password = 'your-personal-access-token;' } } Finally, press Ctrl+o to write your configuration to file. To pull Docker images from GitHub, you also have to configure the credentials for singularity as environment variables. To do that, create a new config file as follows: nano ~/.nextflow/config and write the following parameters, replacing the SINGULARITY_DOCKER_PASSWORD and with the your PAT. env { SINGULARITY_DOCKER_LOGIN='$oauthtoken' SINGULARITY_DOCKER_PASSWORD='your-personal-access-token;' } Finally, press Ctrl+o to write your configuration to file.","title":"Add your GitHub PAT"},{"location":"systems-setup/hpc-cell/#test-your-installation","text":"You can test your installation by creating a test-workflow in your home directory as follows: mkdir ~/test-workflow && cd ~/test-workflow Now pull the boot-nf workflow as follows: nextflow pull stracquadaniolab/boot-nf and run it on Cell as follows: nextflow run stracquadaniolab/boot-nf -profile singularity,slurm,test which will create a file results/results.txt containing your ip address and location in JSON format.","title":"Test your installation"},{"location":"systems-setup/hpc-cell/#faq","text":"I tried to pull a docker image, but it gives me an auth error : 1) you should have a PAT (personal access token) added so that you can access github images github help . 2) you should be added to the docker group by an admin, contact the IS helpline to have your used enable 3) your docker image should be built and pushed to the gh repository (check the github action for the workflow). These three steps should solve 95% of problems. Pipeline suddenly fails with 137 error : 1) Most likely you haven't allocated enough memory for your job. Check Nextflow configuration file for details (particularly executor scope: memory setting).","title":"FAQ"},{"location":"systems-setup/reference-manager/","text":"Reference manager We use Zotero as the standard reference manager for the lab. It is open source, it exports in BibTex and can be used across MacOS, Linux and iOS. Install Zotero Go to the Zotero homepage and download the executable for your machine. Papers storage and synchronization Zotero comes with a very limited amount of space, but it allows to store and synchronize papers across devices using a WebDav server. At this time, the best option is to create a pCloud account, a cloud service that gives you up to 10Gb for free and supports WebDav. To store your papers in pCloud follow these steps: Go to Preferences > Sync . Tick the option Sync attachment files in my library using: and select WebDav. Finally set the WebDav parameters as follows: URL: ewebdav.pcloud.com and https protocol. Username: <your-pcloud-username> Password: <your-pcloud-password> Click Verify server Your papers are now stored and synchronized with pCloud .","title":"Reference manager"},{"location":"systems-setup/reference-manager/#reference-manager","text":"We use Zotero as the standard reference manager for the lab. It is open source, it exports in BibTex and can be used across MacOS, Linux and iOS.","title":"Reference manager"},{"location":"systems-setup/reference-manager/#install-zotero","text":"Go to the Zotero homepage and download the executable for your machine.","title":"Install Zotero"},{"location":"systems-setup/reference-manager/#papers-storage-and-synchronization","text":"Zotero comes with a very limited amount of space, but it allows to store and synchronize papers across devices using a WebDav server. At this time, the best option is to create a pCloud account, a cloud service that gives you up to 10Gb for free and supports WebDav. To store your papers in pCloud follow these steps: Go to Preferences > Sync . Tick the option Sync attachment files in my library using: and select WebDav. Finally set the WebDav parameters as follows: URL: ewebdav.pcloud.com and https protocol. Username: <your-pcloud-username> Password: <your-pcloud-password> Click Verify server Your papers are now stored and synchronized with pCloud .","title":"Papers storage and synchronization"},{"location":"systems-setup/workstation/","text":"Workstations The lab uses Mac workstations running Mac OS X, which come pre-configured to University standards by IT systems. We established a setup procedure to standardise the work environment in the lab and streamline know-how sharing. Upon receiving your machine, you should go through the setup procedure before working on your projects. Update Mac OS X Make sure that you are running latest Mac OS X version before installing any tool. To do that, go to: System Preferences > Software Update . The system will automatically check the presence of new updates and, if so, download and install them by pressing the Update Now button. Install Developer Tools Developers Tools is a package including the basic tools for development on Mac OS X, including gcc , python and git . To install Developer Tools , open a Terminal and run the following command: xcode-select --install press the Install button and agree to the license terms. Install Visual Studio Code Visual Studio Code (VSC) is a powerful open-source IDE, which provides excellent tools for code development, including autocomplete, online debug, and Latex live compilation. To install VSC , download the installer from: https://code.visualstudio.com . Move the application from the Downloads folder to the Applications folder, which you can find in the Finder . Recommended plugins to install: Better comments Code spell checker Docker Git Graph EditorConfig for VSCode Git History Latex workshop Nextflow Remote containers Remote - SSH Rewrap Todo Tree Install Docker The lab uses Docker containers as the underpinning execution system for the projects. You can install the latest release from https://www.docker.com . Install Nextflow Nextflow is the default workflow orchestration system. You can install Nextflow from the command line by running: curl -s https://get.nextflow.io | bash Install miniconda You should use miniconda to install any software you might need, since it allows user-level installations of most common Unix software. If you cannot find your software in the default channel, please check also the conda-forge and bioconda channels. To install miniconda , download the installer in your home directory as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Then, run the installer as follows: bash Miniconda3-latest-Linux-x86_64.sh and answer 'yes' to each question including the default location. Finally, activate your installation by running: source ~/.bashrc Install Pipx Many tools we use are written in Python and, depending on how your system and projects are configured, they might create unwanted problems, which can be easily avoided by using Pipx . To install Pipx , open a Terminal and run the following command: python3 -m pip install --user pipx python3 -m pipx ensurepath Install cookiecutter cookiecutter is a command-line utility that creates folder structures from a template. We use cookiecutters to bootstrap all our projects. To install cookiecutter , open a Terminal and run the following command: pipx install cookiecutter Install bump2version We manage semantic versioning using bump2version, which can be installed by opening a Terminal and run the following command: pipx install bump2version","title":"Workstations"},{"location":"systems-setup/workstation/#workstations","text":"The lab uses Mac workstations running Mac OS X, which come pre-configured to University standards by IT systems. We established a setup procedure to standardise the work environment in the lab and streamline know-how sharing. Upon receiving your machine, you should go through the setup procedure before working on your projects.","title":"Workstations"},{"location":"systems-setup/workstation/#update-mac-os-x","text":"Make sure that you are running latest Mac OS X version before installing any tool. To do that, go to: System Preferences > Software Update . The system will automatically check the presence of new updates and, if so, download and install them by pressing the Update Now button.","title":"Update Mac OS X"},{"location":"systems-setup/workstation/#install-developer-tools","text":"Developers Tools is a package including the basic tools for development on Mac OS X, including gcc , python and git . To install Developer Tools , open a Terminal and run the following command: xcode-select --install press the Install button and agree to the license terms.","title":"Install Developer Tools"},{"location":"systems-setup/workstation/#install-visual-studio-code","text":"Visual Studio Code (VSC) is a powerful open-source IDE, which provides excellent tools for code development, including autocomplete, online debug, and Latex live compilation. To install VSC , download the installer from: https://code.visualstudio.com . Move the application from the Downloads folder to the Applications folder, which you can find in the Finder . Recommended plugins to install: Better comments Code spell checker Docker Git Graph EditorConfig for VSCode Git History Latex workshop Nextflow Remote containers Remote - SSH Rewrap Todo Tree","title":"Install Visual Studio Code"},{"location":"systems-setup/workstation/#install-docker","text":"The lab uses Docker containers as the underpinning execution system for the projects. You can install the latest release from https://www.docker.com .","title":"Install Docker"},{"location":"systems-setup/workstation/#install-nextflow","text":"Nextflow is the default workflow orchestration system. You can install Nextflow from the command line by running: curl -s https://get.nextflow.io | bash","title":"Install Nextflow"},{"location":"systems-setup/workstation/#install-miniconda","text":"You should use miniconda to install any software you might need, since it allows user-level installations of most common Unix software. If you cannot find your software in the default channel, please check also the conda-forge and bioconda channels. To install miniconda , download the installer in your home directory as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Then, run the installer as follows: bash Miniconda3-latest-Linux-x86_64.sh and answer 'yes' to each question including the default location. Finally, activate your installation by running: source ~/.bashrc","title":"Install\u00a0miniconda"},{"location":"systems-setup/workstation/#install-pipx","text":"Many tools we use are written in Python and, depending on how your system and projects are configured, they might create unwanted problems, which can be easily avoided by using Pipx . To install Pipx , open a Terminal and run the following command: python3 -m pip install --user pipx python3 -m pipx ensurepath","title":"Install Pipx"},{"location":"systems-setup/workstation/#install-cookiecutter","text":"cookiecutter is a command-line utility that creates folder structures from a template. We use cookiecutters to bootstrap all our projects. To install cookiecutter , open a Terminal and run the following command: pipx install cookiecutter","title":"Install cookiecutter"},{"location":"systems-setup/workstation/#install-bump2version","text":"We manage semantic versioning using bump2version, which can be installed by opening a Terminal and run the following command: pipx install bump2version","title":"Install bump2version"}]}